{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkaunismaa/PyTorchTutorials/blob/main/PyTorch_2/scaled_dot_product_attention_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wednesday, April 26, 2023\n",
        "\n",
        "This all runs in one pass"
      ],
      "metadata": {
        "id": "-4qBRBAqYhX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0L4jfDGQYB6u"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h26YyQB_YB6y"
      },
      "source": [
        "\n",
        "# (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)\n",
        "\n",
        "\n",
        "**Author:** [Driss Guessous](https://github.com/drisspg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zlvSzIAYB60"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we want to highlight a new ``torch.nn.functional`` function\n",
        "that can be helpful for implementing transformer architectures. The\n",
        "function is named ``torch.nn.functional.scaled_dot_product_attention``.\n",
        "For detailed description of the function, see the [PyTorch documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention)_.\n",
        "This function has already been incorporated into ``torch.nn.MultiheadAttention`` and ``torch.nn.TransformerEncoderLayer``.\n",
        "\n",
        "## Overview\n",
        "At a high level, this PyTorch function calculates the\n",
        "scaled dot product attention (SDPA) between query, key, and value according to\n",
        "the definition found in the paper [Attention is all you\n",
        "need](https://arxiv.org/abs/1706.03762)_. While this function can\n",
        "be written in PyTorch using existing functions, a fused implementation can provide\n",
        "large performance benefits over a naive implementation.\n",
        "\n",
        "## Fused implementations\n",
        "\n",
        "For CUDA tensor inputs, the function will dispatch into one of the following\n",
        "implementations:\n",
        "\n",
        "* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)_\n",
        "* [Memory-Efficient Attention](https://github.com/facebookresearch/xformers)_\n",
        "* A PyTorch implementation defined in C++\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial requires PyTorch 2.0.0 or later.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xqCASeq9YB61",
        "outputId": "5845ecc8-34f0-443f-ea67-85a05214751a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-3.7484e-01, -3.4887e-01, -1.0694e-01,  8.6957e-01, -2.3158e-01,\n",
              "          -1.0520e-01,  4.8912e-01, -1.5781e-01],\n",
              "         [ 7.4625e-02, -6.3294e-01, -7.2413e-01,  5.5303e-01, -1.6446e-01,\n",
              "          -2.5935e-01, -1.9949e-02, -1.6789e-04],\n",
              "         [ 7.2280e-02, -6.5601e-01, -7.0848e-01,  6.8477e-01, -3.1107e-01,\n",
              "          -2.1928e-01,  1.3451e-01, -9.1173e-02]],\n",
              "\n",
              "        [[ 7.0666e-01, -2.3627e-02,  4.1862e-01,  4.6713e-01, -3.5425e-01,\n",
              "          -9.0415e-01,  7.5038e-01, -1.1394e-01],\n",
              "         [ 7.1524e-01, -1.8635e-01,  4.0190e-01,  4.7281e-01, -3.7997e-01,\n",
              "          -8.1729e-01,  5.5356e-01, -4.2449e-02],\n",
              "         [ 1.0052e+00, -2.9670e-01,  2.6351e-01,  2.4091e-01,  6.7387e-02,\n",
              "          -5.6868e-01,  7.3684e-01,  9.1429e-02]]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Example Usage:\n",
        "query, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\n",
        "F.scaled_dot_product_attention(query, key, value)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "uBQWly55YQ-q",
        "outputId": "6fa5127f-ccf0-4453-bb45-08247037aefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57BoEESgYB62"
      },
      "source": [
        "## Explicit Dispatcher Control\n",
        "\n",
        "While the function will implicitly dispatch to one of the three\n",
        "implementations, the user can also explicitly control the dispatch via\n",
        "the use of a context manager. This context manager allows users to\n",
        "explicitly disable certain implementations. If a user wants to ensure\n",
        "the function is indeed using the fastest implementation for their\n",
        "specific inputs, the context manager can be used to sweep through\n",
        "measuring performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zITn3sxoYB62",
        "outputId": "ca38268f-6a93-4aeb-beb1-91bde325d619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The default implementation runs in 13181.939 microseconds\n",
            "The math implementation runs in 42215.553 microseconds\n",
            "The flash attention implementation runs in 13086.654 microseconds\n",
            "The memory efficient implementation runs in 14306.842 microseconds\n"
          ]
        }
      ],
      "source": [
        "# Lets define a helpful benchmarking function:\n",
        "import torch.utils.benchmark as benchmark\n",
        "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
        "    t0 = benchmark.Timer(\n",
        "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
        "    )\n",
        "    return t0.blocked_autorange().mean * 1e6\n",
        "\n",
        "# Lets define the hyper-parameters of our input\n",
        "batch_size = 32\n",
        "max_sequence_len = 1024\n",
        "num_heads = 32\n",
        "embed_dimension = 32\n",
        "\n",
        "dtype = torch.float16\n",
        "\n",
        "query = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
        "key = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
        "value = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
        "\n",
        "print(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
        "\n",
        "# Lets explore the speed of each of the 3 implementations\n",
        "from torch.backends.cuda import sdp_kernel, SDPBackend\n",
        "\n",
        "# Helpful arguments mapper\n",
        "backend_map = {\n",
        "    SDPBackend.MATH: {\"enable_math\": True, \"enable_flash\": False, \"enable_mem_efficient\": False},\n",
        "    SDPBackend.FLASH_ATTENTION: {\"enable_math\": False, \"enable_flash\": True, \"enable_mem_efficient\": False},\n",
        "    SDPBackend.EFFICIENT_ATTENTION: {\n",
        "        \"enable_math\": False, \"enable_flash\": False, \"enable_mem_efficient\": True}\n",
        "}\n",
        "\n",
        "with sdp_kernel(**backend_map[SDPBackend.MATH]):\n",
        "    print(f\"The math implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
        "\n",
        "\n",
        "with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n",
        "    try:\n",
        "        print(f\"The flash attention implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
        "    except RuntimeError:\n",
        "        print(\"FlashAttention is not supported. See warnings for reasons.\")\n",
        "\n",
        "with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n",
        "    try:\n",
        "        print(f\"The memory efficient implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
        "    except RuntimeError:\n",
        "        print(\"EfficientAttention is not supported. See warnings for reasons.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H_WOWUNYB63"
      },
      "source": [
        "## Hardware dependence\n",
        "\n",
        "Depending on what machine you ran the above cell on and what hardware is\n",
        "available, your results might be different.\n",
        "- If you don’t have a GPU and are running on CPU then the context manager\n",
        "will have no effect and all three runs should return similar timings.\n",
        "- Depending on what compute capability your graphics card supports\n",
        "flash attention or memory efficient might have failed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnRiXKLeYB64"
      },
      "source": [
        "## Causal Self Attention\n",
        "\n",
        "Below is an example implementation of a multi-headed causal self\n",
        "attention block inspired by\n",
        "[Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)_ repository.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ACqEIG7jYB65",
        "outputId": "4d59ea31-0991-43e7-f9f7-d42faeab4826",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CausalSelfAttention(\n",
            "  (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n",
            "  (c_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n",
        "        super().__init__()\n",
        "        assert embed_dimension % num_heads == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(embed_dimension, 3 * embed_dimension, bias=bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias)\n",
        "        # regularization\n",
        "        self.dropout = dropout\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dimension = embed_dimension\n",
        "        # Perform causal masking\n",
        "        self.is_causal = is_causal\n",
        "\n",
        "    def forward(self, x):\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        query_projected = self.c_attn(x)\n",
        "\n",
        "        batch_size = query_projected.size(0)\n",
        "        embed_dim = query_projected.size(2)\n",
        "        head_dim = embed_dim // (self.num_heads * 3)\n",
        "\n",
        "        query, key, value = query_projected.chunk(3, -1)\n",
        "        query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
        "        key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        if self.training:\n",
        "            dropout = self.dropout\n",
        "            is_causal = self.is_causal\n",
        "        else:\n",
        "            dropout = 0.0\n",
        "            is_causal = False\n",
        "\n",
        "        y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n",
        "        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n",
        "\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "num_heads = 8\n",
        "heads_per_dim = 64\n",
        "embed_dimension = num_heads * heads_per_dim\n",
        "dtype = torch.float16\n",
        "model = CausalSelfAttention(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to(dtype).eval()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwZgpEZXYB65"
      },
      "source": [
        "### ``NestedTensor`` and Dense tensor support\n",
        "\n",
        "SDPA supports both ``NestedTensor`` and Dense tensor inputs. ``NestedTensors`` handle the case where the input is a batch of variable length sequences\n",
        "without needing to pad each sequence to the maximum length in the batch. For more information about ``NestedTensors`` see\n",
        "[torch.nested](https://pytorch.org/docs/stable/nested.html)_ and [NestedTensors Tutorial](https://pytorch.org/tutorials/prototype/nestedtensor.html)_.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tA88mkp4YB66",
        "outputId": "0234c816-7a16-445f-ae42-eeb9f26ba129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FlashAttention is not supported. See warnings for reasons.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-32efb060875d>:29: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
            "  torch.nested.nested_tensor(\n",
            "<ipython-input-5-16ff9494ff75>:38: UserWarning: Memory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:527.)\n",
            "  y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n",
            "<ipython-input-5-16ff9494ff75>:38: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:338.)\n",
            "  y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n",
            "<ipython-input-5-16ff9494ff75>:38: UserWarning: Flash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:529.)\n",
            "  y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n",
            "<ipython-input-5-16ff9494ff75>:38: UserWarning: We are not enabling nested Tensors for Flash Attention because of cuda memory errors. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:155.)\n",
            "  y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "def generate_rand_batch(\n",
        "    batch_size,\n",
        "    max_sequence_len,\n",
        "    embed_dimension,\n",
        "    pad_percentage=None,\n",
        "    dtype=torch.float16,\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    if not pad_percentage:\n",
        "        return (\n",
        "            torch.randn(\n",
        "                batch_size,\n",
        "                max_sequence_len,\n",
        "                embed_dimension,\n",
        "                dtype=dtype,\n",
        "                device=device,\n",
        "            ),\n",
        "            None,\n",
        "        )\n",
        "    # Random sequence lengths\n",
        "    seq_len_list = [\n",
        "        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))\n",
        "        for _ in range(batch_size)\n",
        "    ]\n",
        "    # Make random entry in the batch have max sequence length\n",
        "    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len\n",
        "    return (\n",
        "        torch.nested.nested_tensor(\n",
        "            [\n",
        "                torch.randn(seq_len, embed_dimension,\n",
        "                            dtype=dtype, device=device)\n",
        "                for seq_len in seq_len_list\n",
        "            ]\n",
        "        ),\n",
        "        seq_len_list,\n",
        "    )\n",
        "\n",
        "random_nt, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, dtype=dtype, device=device)\n",
        "random_dense, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, dtype=dtype, device=device)\n",
        "\n",
        "# Currently the fused implementations don't support ``NestedTensor`` for training\n",
        "model.eval()\n",
        "\n",
        "with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n",
        "    try:\n",
        "        print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model, random_nt):.3f} microseconds\")\n",
        "        print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model, random_dense):.3f} microseconds\")\n",
        "    except RuntimeError:\n",
        "        print(\"FlashAttention is not supported. See warnings for reasons.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf0hQ_NPYB66"
      },
      "source": [
        "# Using SDPA with ``torch.compile``\n",
        "\n",
        "With the release of PyTorch 2.0, a new feature called\n",
        "``torch.compile()`` has been introduced, which can provide\n",
        "significant performance improvements over eager mode.\n",
        "Scaled dot product attention is fully composable with ``torch.compile()``.\n",
        "To demonstrate this, let's compile the ``CausalSelfAttention`` module using\n",
        "``torch.compile()`` and observe the resulting performance improvements.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DOZVqNioYB67",
        "outputId": "2a24ea1f-4a47-4453-9406-210120d77b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The non compiled module runs in  1156.176 microseconds\n",
            "The compiled module runs in  1266.105 microseconds\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "max_sequence_len = 256\n",
        "x = torch.rand(batch_size, max_sequence_len,\n",
        "               embed_dimension, device=device, dtype=dtype)\n",
        "print(\n",
        "    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, x):.3f} microseconds\")\n",
        "\n",
        "\n",
        "compiled_model = torch.compile(model)\n",
        "# Let's compile it\n",
        "compiled_model(x)\n",
        "print(\n",
        "    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(compiled_model, x):.3f} microseconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRPHyttYB67"
      },
      "source": [
        "The exact execution time is dependent on machine, however the results for mine:\n",
        "The non compiled module runs in  166.616 microseconds\n",
        "The compiled module runs in  166.726 microseconds\n",
        "That is not what we were expecting. Let's dig a little deeper.\n",
        "PyTorch comes with an amazing built-in profiler that you can use to\n",
        "inspect the performance characteristics of your code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dMC8ocJ-YB68",
        "outputId": "98a716ce-b6e9-478c-a97b-5b583a0b0e59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Non-Compilied Causal Attention         9.81%       5.453ms        39.31%      21.855ms      21.855ms       0.000us         0.00%      49.650ms      49.650ms             1  \n",
            "                                           aten::matmul         0.85%     473.000us        16.41%       9.124ms     182.480us       0.000us         0.00%      29.137ms     582.740us            50  \n",
            "                                               aten::mm        12.42%       6.907ms        14.76%       8.207ms     164.140us      26.757ms        61.52%      29.137ms     582.740us            50  \n",
            "                                           aten::linear         0.69%     386.000us        17.29%       9.610ms     192.200us       0.000us         0.00%      27.270ms     545.400us            50  \n",
            "         turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us      26.757ms        61.52%      26.757ms     535.140us            50  \n",
            "                     aten::scaled_dot_product_attention         4.11%       2.287ms         9.86%       5.482ms     219.280us       0.000us         0.00%      20.513ms     820.520us            25  \n",
            "              aten::_scaled_dot_product_flash_attention         1.30%     723.000us         5.75%       3.195ms     127.800us       0.000us         0.00%      20.513ms     820.520us            25  \n",
            "                         aten::_flash_attention_forward         0.87%     482.000us         1.87%       1.040ms      41.600us      16.536ms        38.02%      18.715ms     748.600us            25  \n",
            "void fmha_fwd_loop_kernel<FMHA_kernel_traits<256, 64...         0.00%       0.000us         0.00%       0.000us       0.000us      16.536ms        38.02%      16.536ms     661.440us            25  \n",
            "                                           aten::arange         0.84%     466.000us         3.69%       2.051ms      20.510us     200.000us         0.46%       3.596ms      35.960us           100  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 55.591ms\n",
            "Self CUDA time total: 43.493ms\n",
            "\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                              Compiled Causal Attention         7.37%       3.489ms        38.44%      18.191ms      18.191ms       0.000us         0.00%      47.340ms      47.340ms             1  \n",
            "                                       CompiledFunction        16.97%       8.030ms        30.83%      14.591ms     583.640us       0.000us         0.00%      47.340ms       1.894ms            25  \n",
            "                                               aten::mm         2.94%       1.392ms         4.42%       2.094ms      41.880us      26.739ms        61.52%      28.977ms     579.540us            50  \n",
            "         turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us      26.739ms        61.52%      26.739ms     534.780us            50  \n",
            "              aten::_scaled_dot_product_flash_attention         1.08%     512.000us         6.95%       3.287ms     131.480us       0.000us         0.00%      18.363ms     734.520us            25  \n",
            "                         aten::_flash_attention_forward         0.92%     436.000us         2.17%       1.028ms      41.120us      16.528ms        38.02%      17.601ms     704.040us            25  \n",
            "void fmha_fwd_loop_kernel<FMHA_kernel_traits<256, 64...         0.00%       0.000us         0.00%       0.000us       0.000us      16.528ms        38.02%      16.528ms     661.120us            25  \n",
            "                                       cudaLaunchKernel         2.59%       1.227ms         2.59%       1.227ms       9.816us       2.295ms         5.28%       2.295ms      18.360us           125  \n",
            "                                           aten::arange         1.15%     542.000us         4.98%       2.357ms      23.570us     200.000us         0.46%       1.520ms      15.200us           100  \n",
            "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.12%      56.000us         0.12%      56.000us       2.240us     787.000us         1.81%     787.000us      31.480us            25  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 47.326ms\n",
            "Self CUDA time total: 43.467ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "activities = [ProfilerActivity.CPU]\n",
        "if device == 'cuda':\n",
        "    activities.append(ProfilerActivity.CUDA)\n",
        "\n",
        "with profile(activities=activities, record_shapes=False) as prof:\n",
        "    with record_function(\" Non-Compilied Causal Attention\"):\n",
        "        for _ in range(25):\n",
        "            model(x)\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "\n",
        "\n",
        "with profile(activities=activities, record_shapes=False) as prof:\n",
        "    with record_function(\"Compiled Causal Attention\"):\n",
        "        for _ in range(25):\n",
        "            compiled_model(x)\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "\n",
        "# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n",
        "# ::\n",
        "#\n",
        "#    prof.export_chrome_trace(\"compiled_causal_attention_trace.json\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NImDazGhYB68"
      },
      "source": [
        "The previous code snippet generates a report of the top 10 PyTorch functions\n",
        "that consumed the most GPU execution time, for both the compiled and non-compiled module.\n",
        "The analysis reveals that the majority of time spent on the GPU is concentrated\n",
        "on the same set of functions for both modules.\n",
        "The reason for this here is that ``torch.compile`` is very good at removing the\n",
        "framework overhead associated with PyTorch. If your model is launching\n",
        "large, efficient CUDA kernels, which in this case ``CausaulSelfAttention``\n",
        "is, then the overhead of PyTorch can be hidden.\n",
        "\n",
        "In reality, your module does not normally consist of a singular\n",
        "``CausalSelfAttention`` block. When experimenting with [Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)_ repository, compiling\n",
        "the module took the time per train step from: ``6090.49ms`` to\n",
        "``3273.17ms``! This was done on commit: ``ae3a8d5`` of NanoGPT training on\n",
        "the Shakespeare dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5lNG9hAYB68"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this tutorial, we have demonstrated the basic usage of\n",
        "``torch.nn.functional.scaled_dot_product_attention``. We have shown how\n",
        "the ``sdp_kernel`` context manager can be used to assert a certain\n",
        "implementation is used on GPU. As well, we built a simple\n",
        "``CausalSelfAttention`` module that works with ``NestedTensor`` and is torch\n",
        "compilable. In the process we have shown how to the profiling tools can\n",
        "be used to explore the performance characteristics of a user defined\n",
        "module.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "A4RFcuh-YrRj",
        "outputId": "e4ae34b9-e7d8-45f9-826c-e834193635a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 26 11:46:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    27W /  70W |   5493MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6tBExs8Ys2l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}